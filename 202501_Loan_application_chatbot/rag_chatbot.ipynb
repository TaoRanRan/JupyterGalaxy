{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f43c5a0-0e42-4307-b5f0-a9ec0b1c98af",
   "metadata": {},
   "source": [
    " # Loan Application Chatbot\n",
    " This script sets up a chatbot to handle loan application queries using OpenAI's GPT-3.5 and LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ac9d9-386b-44cb-8e3d-6d8a7995774c",
   "metadata": {},
   "source": [
    " ## Table of Contents\n",
    " 1. [Import Libraries](#import-libraries)\n",
    " 2. [Set Up API Keys](#set-up-api-keys)\n",
    " 3. [Configure LLM Model](#configure-llm-model)\n",
    " 4. [Load Document](#load-document)\n",
    "     1. [Load Document](#load-document)\n",
    "     2. [Create Searchable Vector Index](#create-searchable-vector-index)\n",
    " 5. [Initialize LLM](#initialize-llm)\n",
    "     1. [Customer Service Chain](#customer-service-chain)\n",
    "     2. [Q & A Chain](#q-a-chain)\n",
    "     3. [Handle Customer Query](#handle-customer-query)\n",
    " 6. [Testing the Chatbot](#testing-the-chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea1b71-30c4-48d0-b067-29185e2a7b35",
   "metadata": {},
   "source": [
    " ## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e7100-f0a9-4688-85e3-178daab524d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd32aa-56c7-4fb5-9884-8d8686abc21a",
   "metadata": {},
   "source": [
    " ## 2. Set Up API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8f73c-56a1-464b-ae4c-aae12b843b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API key\n",
    "openai_key  = your_openai_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231cfcc-d948-4c0c-81cc-df90c516ae54",
   "metadata": {},
   "source": [
    " ## 3. Configure LLM Model\n",
    " Specify the LLM model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75156c2e-39e4-4ed4-8908-3915daca75d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM model to use\n",
    "llm_model = \"gpt-3.5-turbo-0125\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9221bc-5c1e-4ee6-b813-8a97630e4062",
   "metadata": {},
   "source": [
    " ## 4.1 Load Document\n",
    "\n",
    " Load the loan applications data. This data will be used to create a searchable vector index.\n",
    "\n",
    " For experimentation purposes, we will use a sample dataset with only 3 application records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcee390-9d35-428a-a5d4-ef76cd29eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "# Get the directory of the current script\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))  \n",
    "\n",
    "# Define the CSV file path\n",
    "file_path = os.path.join(script_dir, \"loan_applications.csv\")  \n",
    "\n",
    "# each row in the CSV is treated as a separate document\n",
    "loader = CSVLoader(file_path=file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c68b99-d92c-4c01-9fee-b303ceb37268",
   "metadata": {},
   "source": [
    " ## 4.2 Create Searchable Vector Index\n",
    " Convert text into vector embeddings and create a searchable vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ddfa3f-e064-4a99-a7c0-fb14213a6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_key)\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embedding_model\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55160983-0230-4d66-b3ac-326ccb66d1c0",
   "metadata": {},
   "source": [
    " ## 5. Initialize LLM\n",
    " Initialize the LLM with the specified model and API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cc5b1-695a-4e02-8056-4d8e624c4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, \n",
    "                 model=llm_model,\n",
    "                 openai_api_key=openai_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a2e316-1fbd-44d8-b231-e4df1a9d8c76",
   "metadata": {},
   "source": [
    " ## 5.1 Customer Service Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9777d78-3084-4eda-a4a3-f9427d84e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template for polite and professional responses\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are a professional customer service representative for a financial institution. Your tone should always be polite, clear and precise. \n",
    "\n",
    "Please only if necessary, retrieve application-related data (such as application status, the reason for decline) to give a complete response.\n",
    "To do that, you need to ask for application_id first so that you can locate the correct info.\n",
    "\n",
    "If you don't know the answer, admit it and refer the customer for human support.\n",
    "\n",
    "Customer Query: \"{question}\"\n",
    "\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate(input_variables=[\"question\"], \n",
    "                               template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8cfb8-589e-4f4b-b7a6-7b2c35afd604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let LLM have memory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm,\n",
    "                                          max_token_limit=100)\n",
    "\n",
    "# Customer service chain\n",
    "customer_service_chain = LLMChain(prompt=custom_prompt, \n",
    "                                  llm=llm,\n",
    "                                  memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60838a6-fa6a-48e8-9307-693e004aa5e4",
   "metadata": {},
   "source": [
    " ## 5.2 Q & A Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77f6a3-a48e-42d2-a701-58dea546294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the LLM and retriever into a question-answering pipeline\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\",  # All documents are concatenated into a single string \n",
    "    retriever=index.vectorstore.as_retriever(), \n",
    "    verbose=True,\n",
    "    chain_type_kwargs={\"document_separator\": \"<<<<>>>>>\"}  # Custom separator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746fe6d-df4f-4676-984e-28c5ee2a87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to extract application id from the query so that Q&A chain can retrieve the relevant information\n",
    "def extract_application_id(query):\n",
    "    \"\"\"\n",
    "    Function to extract application_id from the query.\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"query\"],\n",
    "        template=\"Please extract and only return the application ID from the following query: '{query}'. If there is no application id, return 'Missing'.\"\n",
    "    )\n",
    "\n",
    "    application_id_chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "    response = application_id_chain.run({\"query\": query})\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69e1dc-eeab-4d87-8e67-6742307fbe13",
   "metadata": {},
   "source": [
    " ## 5.3. Handle Customer Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831bc8be-2bf4-4420-b3b2-290722818be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_customer_query(customer_query):\n",
    "    \"\"\"\n",
    "    Handles the customer query, processes it using the customer service chain,\n",
    "    and provides the relevant response for the application.\n",
    "    \"\"\"\n",
    "    # Classify if the query is related to an application\n",
    "    prompt_template = \"\"\"\n",
    "    Please classify whether the following query is asking about an application.\n",
    "    If it is, return 'yes'. If not, return 'no'.\n",
    "    Customer query: '{customer_query}'\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(input_variables=[\"customer_query\"], template=prompt_template)\n",
    "    application_classification_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    is_app_query = application_classification_chain.run({\"customer_query\": customer_query}).strip().lower()\n",
    "\n",
    "    # If the query is asking about an application, extract application ID\n",
    "    if is_app_query == \"yes\":\n",
    "        application_id = extract_application_id(customer_query)\n",
    "        \n",
    "        if application_id == 'Missing':\n",
    "            full_response = customer_service_chain.run(customer_query)\n",
    "        else:\n",
    "            # Query qa_chain to retrieve the relevant information\n",
    "            filtered_query = f\"Application ID: {application_id}. What is the status of my application and why?\"\n",
    "            retrieved_data = qa_chain.run(filtered_query)\n",
    "            \n",
    "            # Combine the initial response with the retrieved data \n",
    "            if retrieved_data:\n",
    "                full_response = f\"Thank you for providing your application ID. Here is the specific information for Application ID {application_id}: {retrieved_data}\"\n",
    "            else:\n",
    "                full_response = customer_service_chain.run(customer_query)\n",
    "    else:\n",
    "        # If it doesn't ask about the application\n",
    "        full_response = customer_service_chain.run(customer_query)\n",
    "    \n",
    "    # Update memory \n",
    "    memory.save_context({\"input\": customer_query}, {\"output\": full_response})\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6832b-b235-43f2-891b-361a10ce7ca4",
   "metadata": {},
   "source": [
    " ## 6. Testing the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6281235-b158-49b2-85a6-9cea9640eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, thank you for reaching out to us. In order to provide you with more information about why your application was declined, could you please provide me with your application ID? This will help me locate the correct information for you. If you don't have the application ID handy, I recommend contacting our customer support team for further assistance. Thank you for your understanding.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Thank you for providing your application ID. Here is the specific information for Application ID 2: The status of your application with ID 2 is declined. The reason for the decline is \"Applicant UC score greater than risk cut-off; Affordability < 0.\"\n",
      "Hello Andy, thank you for reaching out to us. How can I assist you today? If you have a specific question or concern regarding your application or account, please provide me with your application ID so that I can locate the necessary information for you.\n",
      "Thank you for reaching out to us. I'm sorry to hear about your lost credit card. To assist you further, may I please have your application_id so that I can locate your account information and provide you with the necessary support? If you don't have your application_id handy, I recommend contacting our customer support team for immediate assistance. Thank you for your understanding.\n",
      "I'm sorry to hear that you're not happy. Can you please provide me with your application_id so that I can look into the issue and assist you further? If you don't have the application_id handy, I recommend reaching out to our customer support team for personalized assistance. Thank you for your understanding.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Thank you for providing your application ID. Here is the specific information for Application ID 3: The status of your application with ID 3 is declined. The reason for the decline is \"Missing last year income.\"\n",
      "Hello Andy, thank you for reaching out to us. How can I assist you today? If you have a specific question regarding an application or account, please provide me with your application ID so that I can locate the necessary information for you.\n",
      "Thank you for reaching out. In order to assist you with your query, may I please have your application_id so that I can retrieve the necessary information? If you do not have this information readily available, I recommend contacting our customer support team for further assistance. Thank you for your understanding.\n"
     ]
    }
   ],
   "source": [
    "# Sample queries\n",
    "print(handle_customer_query(\"Why was my application declined?\"))\n",
    "print(handle_customer_query(\"my application id is 2\"))\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "print(handle_customer_query(\"Hi, my name is andy\"))\n",
    "print(handle_customer_query(\"I lost my credit card\"))\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "print(handle_customer_query(\"I am not happy\"))\n",
    "print(handle_customer_query(\"Why my application was declined. My application_id is 3\"))\n",
    "memory.load_memory_variables({})\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "print(handle_customer_query(\"Hi, my name is andy\"))\n",
    "print(handle_customer_query(\"What is my name?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbotenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
